{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border:solid 1px gray;\">\n",
    "    <a href=\"https://openalex.org/\">\n",
    "        <img src=\"../../../resources/img/OpenAlex-banner.png\" alt=\"OpenAlex banner\" width=\"300\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAlex API Webinar - Tutorial 01 - Getting data about papers that a university's research has cited\n",
    "April 25, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Jupyter Notebook accompanying the first OpenAlex webinar on using Python to access the API.\n",
    "\n",
    "* If you aren't familiar with Jupyter notebooks, [you can learn more here](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)\n",
    "* To learn all about the OpenAlex API: [visit the technical documentation](https://docs.openalex.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAlex API is very powerful, but it is also very easy to use. There is no authentication required, and all your code needs to do is make standard HTTP GET requests.\n",
    "\n",
    "While there are some good libraries you can use to access the API, we're going to start very simply by making API calls directly. We will import just two small libraries to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup: import libraries\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set your email here in order to use the API's \"polite pool\"\n",
    "# See: https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool\n",
    "mailto = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.openalex.org/works\"\n",
    "if not mailto:\n",
    "    raise ValueError(\"You need to fill in your email address in the `mailto` variable above!\")\n",
    "params = {\n",
    "    \"mailto\": mailto,\n",
    "    \"filter\": \"authorships.author.id:a5086928770\",  # Kyle Demes's author ID\n",
    "}\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# A \"200\" status code means that the API query was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 24\n"
     ]
    }
   ],
   "source": [
    "results = response.json()['results']\n",
    "print(f\"Number of results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've retrieved the papers from the API, using a simple query:\n",
    "\n",
    "[`https://api.openalex.org/works?filter=authorships.author.id:a5086928770`](https://api.openalex.org/works?filter=authorships.author.id:a5086928770)\n",
    "\n",
    "You can follow that link in the browser to get the same result. But with the data now accessible by our code, we can save a CSV file with whatever fields we want.\n",
    "\n",
    "Instructions for how to write CSV files with Python are [here](https://docs.python.org/3/library/csv.html). Following these instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python documentation shows how to write data to a CSV file:\n",
    "# https://docs.python.org/3/library/csv.html\n",
    "with open('kdemes_works.csv', 'w', newline='') as f:\n",
    "    # initialize the csv writer for this file\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a header row at the top\n",
    "    header = ['id', 'doi', 'publication_year', 'title']\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # loop through the works and write each row\n",
    "    for item in results:\n",
    "        this_id = item['id']\n",
    "        this_doi = item['doi']\n",
    "        this_publication_year = item['publication_year']\n",
    "        this_title = item['title']\n",
    "        writer.writerow([this_id, this_doi, this_publication_year, this_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University (Institution)\n",
    "\n",
    "Next, we'll try something a little more advanced. We're going to get the works from a certain institution, and then retrieve all of the references from those works (the works cited by the university's works).\n",
    "\n",
    "We'll start by collecting the university's works. We'll limit to just recently published papers so it doesn't take too long, but you could get all of the papers just as easily, if you're willing to wait.\n",
    "\n",
    "#### Cursor paging\n",
    "\n",
    "Each API query will only return a limited subset of the overall data, in what is known as a page. We need to make multiple queries to \"page through\" all of the data, collecting the data for each API query. We use a method called [\"cursor paging\"](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done paging through results. We made 44 API queries, and retrieved 4254 results.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.openalex.org/works\"\n",
    "params = {\n",
    "    \"mailto\": \"jportenoy@ourresearch.org\",\n",
    "    \"filter\": f\"authorships.institutions.lineage:i129801699,publication_year:>2022\",  # University of Tasmania\n",
    "    \"per-page\": 100,\n",
    "    \"select\": \"id,doi,publication_year,title,primary_location,authorships,topics\",\n",
    "}\n",
    "\n",
    "# Initialize cursor\n",
    "cursor = \"*\"\n",
    "\n",
    "# Loop through pages\n",
    "all_results = []\n",
    "count_api_queries = 0\n",
    "while cursor:\n",
    "    params[\"cursor\"] = cursor\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Oh no! Something went wrong during the live demo! How embarrassing!\")\n",
    "        break\n",
    "    this_page_results = response.json()['results']\n",
    "    for result in this_page_results:\n",
    "        all_results.append(result)\n",
    "    count_api_queries += 1\n",
    "\n",
    "    # Update cursor, using the response's `next_cursor` metadata field\n",
    "    cursor = response.json()['meta']['next_cursor']\n",
    "print(f\"Done paging through results. We made {count_api_queries} API queries, and retrieved {len(all_results)} results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to loop through each work collected above, and collect all of the referenced works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make our cursor paging code above into a function, so we can reuse it easily.\n",
    "# This code just defines the function. We'll need to call the function later on to get it to actually get it to run.\n",
    "def api_query_page_results(url, params):\n",
    "    # Initialize cursor\n",
    "    cursor = \"*\"\n",
    "\n",
    "    # Loop through pages\n",
    "    all_results = []\n",
    "    while cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Oh no! Something went wrong during the live demo! How embarrassing!\")\n",
    "            response.raise_for_status()\n",
    "        this_page_results = response.json()['results']\n",
    "        for result in this_page_results:\n",
    "            all_results.append(result)\n",
    "\n",
    "        # Update cursor\n",
    "        cursor = response.json()['meta']['next_cursor']\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done collecting references. We retrieved 9132 works.\n"
     ]
    }
   ],
   "source": [
    "# collect all of the works referenced by the works found above\n",
    "# This will be a dictionary mapping Citing Paper -> List of Cited Papers\n",
    "all_references = {}\n",
    "\n",
    "# Let's limit the results to loop through to only n=100, because this is a demo, and we don't want to wait for too long\n",
    "works_to_collect = all_results[:100]\n",
    "\n",
    "# We will keep track of the number of works retrieved from the API\n",
    "count_works_retrieved = 0\n",
    "\n",
    "for work in works_to_collect:\n",
    "    # Get references to this work (i.e., works that have been cited by this work)\n",
    "    this_work_id = work['id']\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "    params = {\n",
    "        \"mailto\": \"jportenoy@ourresearch.org\",\n",
    "        \"filter\": f\"cited_by:{this_work_id}\",\n",
    "        \"per-page\": 100,\n",
    "        \"select\": \"id,doi,publication_year,title,primary_location,authorships,topics\",\n",
    "    }\n",
    "    this_work_references = api_query_page_results(url, params=params)\n",
    "    # put this data into our dictionary:\n",
    "    all_references[this_work_id] = this_work_references\n",
    "    count_works_retrieved += len(this_work_references)\n",
    "print(f\"Done collecting references. We retrieved {count_works_retrieved} works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shorten the OpenAlex ID to make it better for display\n",
    "def make_short_id(long_id):\n",
    "    short_id = long_id.replace(\"https://openalex.org/\", \"\")\n",
    "    return short_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each citing -> cited pair of works to a CSV file\n",
    "output_filename = \"tasmania_paper_references.csv\"\n",
    "with open(output_filename, 'w', newline='') as f:\n",
    "    # initialize the csv writer for this file\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a header row at the top\n",
    "    header = ['citing_paper_id', 'cited_paper_id']\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # loop through each citation, writing one row for each citation\n",
    "    for citing_id, cited_works in all_references.items():\n",
    "        citing_id_short = make_short_id(citing_id)\n",
    "        for cited_work in cited_works:\n",
    "            cited_id_short = make_short_id(cited_work['id'])\n",
    "            writer.writerow([citing_id_short, cited_id_short])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can keep track of how many times each work has been cited.\n",
    "# One way to do this is to use Python's collections.Counter\n",
    "from collections import Counter\n",
    "citation_counts = Counter()\n",
    "for citing_id, cited_works in all_references.items():\n",
    "    citation_counts.update([w['id'] for w in cited_works])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"tasmania_references_paper_metadata.csv\"\n",
    "seen_work_ids = set()\n",
    "with open(output_filename, 'w', newline='') as f:\n",
    "    # initialize the csv writer for this file\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a header row at the top\n",
    "    header = ['work_id', 'title', 'doi', 'utasmania_citation_count', \n",
    "              'source_id', 'source_issn', 'source_display_name', \n",
    "              'primary_topic_id', 'primary_topic_display_name']\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for cited_works in all_references.values():\n",
    "        for w in cited_works:\n",
    "            work_id = w['id']\n",
    "            work_id_short = make_short_id(work_id)\n",
    "            title = w['title']\n",
    "            if work_id not in seen_work_ids and title != 'Deleted Work':\n",
    "                # Write a row to the CSV file for this work\n",
    "                doi = w['doi']\n",
    "                utasmania_citation_count = citation_counts[work_id]\n",
    "\n",
    "                # Get source (journal)\n",
    "                try:\n",
    "                    source = w['primary_location']['source']\n",
    "                    source_id = source['id']\n",
    "                    source_id_short = make_short_id(source_id)\n",
    "                    source_issn = source['issn_l']\n",
    "                    source_display_name = source['display_name']\n",
    "                except (KeyError, TypeError):\n",
    "                    source_id = None\n",
    "                    source_issn = None\n",
    "                    source_display_name = None\n",
    "\n",
    "                # Get primary_topic\n",
    "                try:\n",
    "                    primary_topic = w['topics'][0]\n",
    "                    primary_topic_id = primary_topic['id']\n",
    "                    primary_topic_id_short = make_short_id(primary_topic_id)\n",
    "                    primary_topic_display_name = primary_topic['display_name']\n",
    "                except (IndexError, KeyError, TypeError):\n",
    "                    primary_topic_id = None\n",
    "                    primary_topic_display_name = None\n",
    "                \n",
    "                writer.writerow([work_id_short, title, doi, \n",
    "                                 utasmania_citation_count, source_id_short, \n",
    "                                 source_issn, source_display_name, \n",
    "                                 primary_topic_id_short, primary_topic_display_name])\n",
    "            \n",
    "                seen_work_ids.add(work_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two CSV files:\n",
    "\n",
    "* `tasmania_paper_references.csv` has a two column edge-list of citing work -> cited work\n",
    "* `tasmania_references_paper_metadata.csv` has metadata about each cited work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
